{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 銀イルカの3人のセリフをスクレイピングして，同じところと違うところを"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とりま wikiとかから智絵里のセリフをスクレイピングする． \n",
    "\n",
    "テキスト解析 -> 出現頻度，\n",
    "\n",
    "\n",
    "セリフをword2vecで学習して，分散表現を計算できるようにする\n",
    "\n",
    "分散表現を次元削減して2次元座標上にプロットすることで，単語ごとの類似度を可視化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "銀イルカ3人のセリフをスクレイピング  \n",
    "\n",
    "doc2vecで学習して，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirementis\n",
    "\n",
    "- python 3.7.1\n",
    "- pipenv\n",
    "- mecab\n",
    "    - ipa\n",
    "    - neologd\n",
    "    \n",
    "usage: `pipenv install`  \n",
    "python, pipenv, pyenv, mecabのインストール方法は省略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デレステセリフのスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes_sute(q):\n",
    "    \"\"\"\n",
    "    デレステで公開されている各アイドルのセリフデータを取得する\n",
    "    [arg]\n",
    "    q: int\n",
    "    [ret]\n",
    "    df: DataFrame\n",
    "    \"\"\"\n",
    "    html = f\"https://starlight.kirara.ca/char/{q}\"\n",
    "    r = requests.get(html)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    texts = soup.find_all(\"span\")\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        corpus.append(text.text)\n",
    "    df = pd.DataFrame(corpus)\n",
    "    #df[0].value_counts().head(10) # 値の出現頻度をカウント\n",
    "    df = df[~df.duplicated()] # 重複行をdataframeから除外\n",
    "    df = df[~df[0].str.match(\"^[\\s|a-zA-Z|<|\\+|\\d$]\")] # 正規表現にマッチするものを除外\n",
    "    df = df[df[0] != \"\"]\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モバマスセリフのスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes_moba(query=\"緒方智絵里\", debug=False):\n",
    "    \"\"\"\n",
    "    モバマスで公開されているセリフ（含 アイドル名、カード名 など）\n",
    "    をスクレイピングして pandas dataframe にする\n",
    "    \n",
    "    [args]\n",
    "    query: str 省略すると 緒方智絵里 のセリフをスクレイピングする\n",
    "    debug: Bool Trueにするとスクレイピング数を上限2にする。それだけ もっと良い使い方あるはず...\n",
    "    [ret]\n",
    "    df: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    html = f\"https://icws.indigo-bell.com/search?q={query}&st=n\"\n",
    "    r = requests.get(html)\n",
    "    r.encoding = r.apparent_encoding\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    a = soup.find(\"a\", string=\"Last　»\").get(\"href\")\n",
    "    if debug == True:\n",
    "        n = 2\n",
    "    else:\n",
    "        n = int(re.search(r\"page=(\\d+)&\", a).group(1)) # クロールするページ数\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for i in range(n): # セリフが登録されてるページ分ループする\n",
    "        print(f\"crawling page: {i + 1}/{n}\")\n",
    "        html = f\"https://icws.indigo-bell.com/search?page={i + 1}&q={query}&st=n\"\n",
    "        r = requests.get(html)\n",
    "        r.encoding = r.apparent_encoding\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        table = soup.find_all(\"tbody\",class_=\"result\")[0]\n",
    "        rows = table.find_all(\"tr\")\n",
    "        tmp = []\n",
    "        tm = 5 + random.randint(0, 5)\n",
    "\n",
    "        for row in rows:\n",
    "            cell_txt = []\n",
    "            for cell in row.find_all([\"td\", \"th\"]):\n",
    "                cell_txt.append(cell.text)\n",
    "            tmp.append(cell_txt)\n",
    "        df = df.append(tmp)\n",
    "        #print(f\"sleep: {tm}\")\n",
    "        if i != (n - 1):\n",
    "            time.sleep(tm)\n",
    "        else:\n",
    "            continue\n",
    "    return df.reset_index(drop=True) # indexを振り直してもとのindexも削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deresute id\n",
    "# chieri : 112\n",
    "# minami : 185\n",
    "# yui    : 246\n",
    "# sae    : 132\n",
    "# hotaru : 161\n",
    "# yoshino: 262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nai\n",
      "crawling page: 1/20\n",
      "crawling page: 2/20\n",
      "crawling page: 3/20\n",
      "crawling page: 4/20\n",
      "crawling page: 5/20\n",
      "crawling page: 6/20\n",
      "crawling page: 7/20\n",
      "crawling page: 8/20\n",
      "crawling page: 9/20\n",
      "crawling page: 10/20\n",
      "crawling page: 11/20\n",
      "crawling page: 12/20\n",
      "crawling page: 13/20\n",
      "crawling page: 14/20\n",
      "crawling page: 15/20\n",
      "crawling page: 16/20\n",
      "crawling page: 17/20\n",
      "crawling page: 18/20\n",
      "crawling page: 19/20\n",
      "crawling page: 20/20\n"
     ]
    }
   ],
   "source": [
    "idol = [\"chieri\", \"112\", \"緒方智絵里\"]\n",
    "\n",
    "# data/に idol_moba.csv idol_dere.csvが両方なかったらスクレイピングを実行する\n",
    "# あったらdataframeにする\n",
    "if not glob.glob(f\"data/{idol[0]}_moba.csv\") and not glob.glob(f\"data/{idol[0]}_dere.csv\"):\n",
    "    print(\"nai\")\n",
    "    aa = get_quotes_sute(int(idol[1]))\n",
    "    bb = get_quotes_moba(query=idol[2])\n",
    "    aa.to_csv(f\"data/{idol[0]}_moba.csv\")\n",
    "    bb.to_csv(f\"data/{idol[0]}_dere.csv\")\n",
    "else:\n",
    "    print(\"aru\")\n",
    "    aa = pd.read_csv(f\"data/{idol[0]}_moba.csv\")\n",
    "    bb = pd.read_csv(f\"data/{idol[0]}_dere.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モバ，ステの結合と正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb[3] = bb[3].str.replace(\"○○\", \"\") # OO (プロデューサー名)部分を落とす\n",
    "bb[3] = bb[3].str.replace(\"〇〇\", \"\") # OO (プロデューサー名)部分を落とす\n",
    "# aa + bb のコーパスを作る\n",
    "docs = pd.Series()\n",
    "docs = pd.concat([aa[0], bb[3]])\n",
    "docs = docs.str.replace(\"プロデューサー\", \"PRODUCER\")\n",
    "docs = docs.str.replace(\"[Producer]\", \"PRODUCER\", regex=False)\n",
    "docs = docs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wani/sandbox/idol_info/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "docs.to_csv(f'data/{idol[0]}_corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mecabを使う。インストールしていなければめかぶをインストール\n",
    "\n",
    "neeologdまでインストールする手順はの参考は\n",
    "- [ubuntu 18.10 に mecab をインストール](https://qiita.com/ekzemplaro/items/c98c7f6698f130b55d53)\n",
    "- [WSLでMeCabとJupyter Notebookを動かす](https://qiita.com/hironobukawaguchi3/items/61a8664dd26c1da66880)\n",
    "- [MecabをPythonで使うまで](https://qiita.com/Sak1361/items/47e9ec464ccc770cd65c)\n",
    "\n",
    "---\n",
    "\n",
    "$ mecab -d /usr/lib/mecab/dic/mecab-ipadic-neologd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_path = !echo `mecab-config --dicdir`\n",
    "def extractNoun(text,\n",
    "                mecab_mode = rf'-Owakati -d {mecab_path[0]}/mecab-ipadic-neologd',\n",
    "                noun=\"名詞\"):\n",
    "    \"\"\"\n",
    "    [arg]: sentence\n",
    "    [ret]: a list of noun words\n",
    "    \"\"\"\n",
    "    print(mecab_mode)\n",
    "    tagger = MeCab.Tagger(mecab_mode)\n",
    "    node = tagger.parseToNode(text)\n",
    "    keywords = []\n",
    "    while node:\n",
    "        surface = node.surface # 表層形を表示(str)\n",
    "        meta = node.feature.split(\",\") # ['記号', '読点', '*', '*', '*', '*', '、', '、', '、']\n",
    "        if meta[0] == noun:\n",
    "            keywords.append(node.surface)\n",
    "        node = node.next # 次のノードのアドレスを指定\n",
    "    return keywords\n",
    "\n",
    "def tokenize(text, wakati=True, dic=\"neologd\"):\n",
    "    \"\"\"\n",
    "    テキストを形態素解析して分かち書きする\n",
    "    [arg]\n",
    "    text: str\n",
    "    [ret]\n",
    "    term: str\n",
    "    \"\"\"\n",
    "    import MeCab\n",
    "    if dic == \"neologd\":\n",
    "        mecab_mode = MeCab.Tagger(rf'-Owakati -d {mecab_path[0]}/mecab-ipadic-neologd') # debugging\n",
    "    else:\n",
    "        mecab_mode = MeCab.Tagger(r'-Owakati')\n",
    "    if wakati == True: # 分かち書きしたものをそのまま取り出す\n",
    "        term = mecab_mode.parse(text).strip()\n",
    "    else: # 品詞で\n",
    "        term = extractNoun(text, mecab_mode)\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a4579841c659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"テキストを形態素解析して分かち書きする\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextractNoun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-761aa8adb3d0>\u001b[0m in \u001b[0;36mextractNoun\u001b[0;34m(text, mecab_mode, noun)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmecab_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmecab_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sandbox/idol_info/.venv/lib/python3.7/site-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_mecabrc_for_bundled_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "text = \"テキストを形態素解析して分かち書きする\"\n",
    "print(extractNoun(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-459f21fb705c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmecab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmecab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"私はペンを持っています。\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sandbox/idol_info/.venv/lib/python3.7/site-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_mecabrc_for_bundled_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mecab = MeCab.Tagger(r\"-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "print(mecab.parse(\"私はペンを持っています。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ipa-dic    ： {tokenize(\"特急はくたか\", dic = \"ipa\")}')\n",
    "print(f'neologd-dic： {tokenize(\"特急はくたか\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [tokenize(row).split() for row in docs]\n",
    "dic = corpora.Dictionary(docs_list)\n",
    "dic.save_as_text('./data/text.dict')  # 保存\n",
    "#gensim.corpora.Dictionary.load_from_text('./data/text.dict')  # 次回からファイルロード可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dic)) # dictionaryに登録された単語(形態素)数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語出現頻度を数える\n",
    "# 余談: bashだと $ sort -t$'\\t' -k3 -nr text.dict で単語とその出現頻度回数カウントできる\n",
    "from collections import defaultdict\n",
    "freq = defaultdict(int)\n",
    "\n",
    "for text in docs_list:\n",
    "    for token in text:\n",
    "        freq[token] += 1\n",
    "print(sorted(freq.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dic.doc2bow(doc) for doc in docs_list] # コーパスの作成\n",
    "corpora.MmCorpus.serialize('./data/text.mm', corpus)  # 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# 折角計算したのでpickleに保存\n",
    "import pickle\n",
    "with open('./data/corpus_tfidf.dump', mode='wb') as f:\n",
    "    pickle.dump(corpus_tfidf, f)\n",
    "\n",
    "# 次回からはロードできます\n",
    "# with open('./data/corpus_tfidf.dump', mode='rb') as f:\n",
    "#     corpus_tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書量にもよりますが、数時間程度かかったりもします。\n",
    "# '18/12/03追記: LdaMulticore で worker の数を増やせばかなり早くなるかもです\n",
    "lda = models.LdaModel(corpus=corpus_tfidf, id2word=dic,\n",
    "                             num_topics=50, minimum_probability=0.001,\n",
    "                             passes=20, update_every=0, chunksize=10000)\n",
    "lda.save('./data/lda.model')  # 保存\n",
    "#lda = gensim.models.LdaModel.load('./data/lda.model')  # 次回からロード可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('tpc_{0}: {1}'.format(i, lda.print_topic(i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
